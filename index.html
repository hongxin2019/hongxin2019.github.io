<!DOCTYPE html>
<html data-html-server-rendered="true" lang="en" data-vue-tag="%7B%22lang%22:%7B%22ssr%22:%22en%22%7D%7D">
  <head>
    <title>Xin Hong (ICT)</title><meta name="gridsome:hash" content="2541bce3e441696c5ad1c5e9e00613e846453e7b"><meta data-vue-tag="ssr" charset="utf-8"><meta data-vue-tag="ssr" name="generator" content="Gridsome v0.7.23"><meta data-vue-tag="ssr" data-key="viewport" name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover"><meta data-vue-tag="ssr" data-key="format-detection" name="format-detection" content="telephone=no"><meta data-vue-tag="ssr" name="viewport" content="width=device-width, initial-scale=1"><meta data-vue-tag="ssr" data-key="description" name="description" content="Xin Hong is a Ph.D. student advised by Yanyan Lan at the Institute of Computing Technology (ICT), Chinese Academy of Sciences. I work in the areas of computer vision and machine learning with focus on visual reasoning, and joint processing of vision and language. Before joining Yanyan Lan&#x27;s research group, I was an research intern at Megvii Technology working on image inpainting."><link data-vue-tag="ssr" rel="icon" href="data:,"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="16x16" href="/assets/static/favicon.ce0531f.c3a70b96b7e4797bf7eec6c39c6e02b6.png"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="32x32" href="/assets/static/favicon.ac8d93a.c3a70b96b7e4797bf7eec6c39c6e02b6.png"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="96x96" href="/assets/static/favicon.b9532cc.c3a70b96b7e4797bf7eec6c39c6e02b6.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="76x76" href="/assets/static/favicon.f22e9f3.c3a70b96b7e4797bf7eec6c39c6e02b6.png"><link rel="preload" href="/assets/css/3.styles.7a4288e4.css" as="style"><link rel="preload" href="/assets/js/app.967b8a34.js" as="script"><link rel="preload" href="/assets/js/page--src--pages--index-vue.d20a73fd.js" as="script"><link rel="prefetch" href="/assets/js/page--node-modules--gridsome--app--pages--404-vue.fbccba8a.js"><link rel="stylesheet" href="/assets/css/3.styles.7a4288e4.css"><noscript data-vue-tag="ssr"><style>.g-image--loading{display:none;}</style></noscript>
  </head>
  <body >
    <div data-server-rendered="true" id="app" class="font-serif"><div class="border-b border-gray-300"><div class="flex flex-wrap px-5 py-16 mx-auto max-w-3xl sm:flex-no-wrap"><div class="w-40 mr-10"><img src="/assets/img/hongxin.25406a49.jpg" class="rounded shadow-lg object-contain"></div><div><p class="text-2xl mt-8 sm:mt-2 font-sans font-bold"><span>Xin Hong</span><span> ( 洪鑫 )</span></p><p class="text-blue-900 font-medium font-mono">
          hongxin19b@ict.ac.cn
        </p><p class="mt-5 max-w-sm sm:max-w-md">I am a Ph.D. student advised by <a class="link" href="http://www.bigdatalab.ac.cn/~lanyanyan/">Yanyan Lan</a> at the Institute of Computing Technology (ICT), Chinese Academy of Sciences. I work in the areas of computer vision and machine learning with focus on visual reasoning, and joint processing of text and image data.
</p></div></div></div><div class="border-b border-gray-300"><div class="px-5 py-16 max-w-3xl mx-auto"><p class="text-2xl mb-8 font-bold font-sans">Publications</p><div class="mt-8"><div class="sm:flex"><div class="w-64 sm:w-64"><a href="https://arxiv.org/pdf/2103.06561.pdf"><img src="/assets/img/brivl.68921958.png" class="object-contain"></a></div><div class="mt-2 sm:mt-0 sm:ml-5 w-full"><p class="font-bold"><span>WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training</span></p><div class="text-sm"><p class="italic text-gray-700"><span><span>Yuqi Huo</span><span>, </span></span><span><span>Manli Zhang</span><span>, </span></span><span><span>Guangzhen Liu</span><span>, </span></span><span><span>Haoyu Lu</span><span>, </span></span><span><span>Yizhao Gao</span><span>, </span></span><span><span>Guoxing Yang</span><span>, </span></span><span><span>Jingyuan Wen</span><span>, </span></span><span><span>Heng Zhang</span><span>, </span></span><span><span>Baogui Xu</span><span>, </span></span><span><span>Weihao Zheng</span><span>, </span></span><span><span>Zongzheng Xi</span><span>, </span></span><span><span>Yueqian Yang</span><span>, </span></span><span><span>Anwen Hu</span><span>, </span></span><span><span>Jinming Zhao</span><span>, </span></span><span><span>Ruichen Li</span><span>, </span></span><span><span>Yida Zhao</span><span>, </span></span><span><span>Liang Zhang</span><span>, </span></span><span><span>Yuqing Song</span><span>, </span></span><span><span class="font-bold text-blue-900">Xin Hong</span><span>, </span></span><span><span>Wanqing Cui</span><span>, </span></span><span><span>Danyang Hou</span><span>, </span></span><span><span>Yingyan Li</span><span>, </span></span><span><span>Junyi Li</span><span>, </span></span><span><span>Peiyu Liu</span><span>, </span></span><span><span>Zheng Gong</span><span>, </span></span><span><span>Chuhao Jin</span><span>, </span></span><span><span>Yuchong Sun</span><span>, </span></span><span><span>Shizhe Chen</span><span>, </span></span><span><span>Zhiwu Lu</span><span>, </span></span><span><span>Zhicheng Dou</span><span>, </span></span><span><span>Qin Jin</span><span>, </span></span><span><span>Yanyan Lan</span><span>, </span></span><span><span>Wayne Xin Zhao</span><span>, </span></span><span><span>Ruihua Song</span><span>, </span></span><span><span>Ji-Rong Wen</span><span>.</span></span></p><p class="text-gray-700"><span>Tec Report</span><span><!----><span>, 2021</span></span><!----><span>.</span></p></div><div class="flex text-lg text-gray-900"><a href="https://arxiv.org/pdf/2103.06561.pdf" class="block mr-2 hover:text-blue-800"><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" class="svg-inline--fa fa-file-pdf fa-w-12"><path fill="currentColor" d="M369.9 97.9L286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"></path></svg></a><!----><!----></div></div></div><div class="border-b h-8 w-full"></div></div><div class="mt-8"><div class="sm:flex"><div class="w-64 sm:w-64"><a href="https://hongxin2019.github.io/TVR"><img src="/assets/img/tvr.0166b70c.png" class="object-contain"></a></div><div class="mt-2 sm:mt-0 sm:ml-5 w-full"><p class="font-bold"><span>Transformation Driven Visual Reasoning</span></p><div class="text-sm"><p class="italic text-gray-700"><span><span class="font-bold text-blue-900">Xin Hong</span><span>, </span></span><span><span>Yanyan Lan</span><span>, </span></span><span><span>Liang Pang</span><span>, </span></span><span><span>Jiafeng Guo</span><span>, </span></span><span><span>Xueqi Cheng</span><span>.</span></span></p><p class="text-gray-700"><span>The IEEE Conference on Computer Vision and Pattern Recognition</span><span><span class="font-bold text-blue-900">
                    (CVPR)</span><span>, 2021</span></span><!----><span>.</span></p></div><div class="flex text-lg text-gray-900"><a href="https://arxiv.org/pdf/2011.13160.pdf" class="block mr-2 hover:text-blue-800"><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" class="svg-inline--fa fa-file-pdf fa-w-12"><path fill="currentColor" d="M369.9 97.9L286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"></path></svg></a><a href="https://github.com/hughplay/TVR" class="block mr-2 hover:text-blue-800"><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" class="svg-inline--fa fa-github fa-w-16"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://hongxin2019.github.io/TVR" class="block mr-2 hover:text-blue-800"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="home" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512" class="svg-inline--fa fa-home fa-w-18"><path fill="currentColor" d="M280.37 148.26L96 300.11V464a16 16 0 0 0 16 16l112.06-.29a16 16 0 0 0 15.92-16V368a16 16 0 0 1 16-16h64a16 16 0 0 1 16 16v95.64a16 16 0 0 0 16 16.05L464 480a16 16 0 0 0 16-16V300L295.67 148.26a12.19 12.19 0 0 0-15.3 0zM571.6 251.47L488 182.56V44.05a12 12 0 0 0-12-12h-56a12 12 0 0 0-12 12v72.61L318.47 43a48 48 0 0 0-61 0L4.34 251.47a12 12 0 0 0-1.6 16.9l25.5 31A12 12 0 0 0 45.15 301l235.22-193.74a12.19 12.19 0 0 1 15.3 0L530.9 301a12 12 0 0 0 16.9-1.6l25.5-31a12 12 0 0 0-1.7-16.93z"></path></svg></a></div></div></div><div class="border-b h-8 w-full"></div></div><div class="mt-8"><div class="sm:flex"><div class="w-64 sm:w-64"><a href="/pdf/mm-2019-dfnet.pdf"><img src="/assets/img/fusion-block.6fb11b23.png" class="object-contain"></a></div><div class="mt-2 sm:mt-0 sm:ml-5 w-full"><p class="font-bold"><span>Deep Fusion Network for Image Completion</span></p><div class="text-sm"><p class="italic text-gray-700"><span><span class="font-bold text-blue-900">Xin Hong</span><span>, </span></span><span><span>Pengfei Xiong</span><span>, </span></span><span><span>Haoqiang Fan</span><span>.</span></span></p><p class="text-gray-700"><span>ACM International Conference on Multimedia</span><span><span class="font-bold text-blue-900">
                    (ACMMM)</span><span>, 2019</span></span><!----><span>.</span></p></div><div class="flex text-lg text-gray-900"><a href="/pdf/mm-2019-dfnet.pdf" class="block mr-2 hover:text-blue-800"><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" class="svg-inline--fa fa-file-pdf fa-w-12"><path fill="currentColor" d="M369.9 97.9L286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"></path></svg></a><a href="https://github.com/hughplay/DFNet" class="block mr-2 hover:text-blue-800"><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" class="svg-inline--fa fa-github fa-w-16"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><!----></div></div></div><div class="border-b h-8 w-full"></div></div><div class="mt-8"><div class="sm:flex"><div class="w-64 sm:w-64"><a href="/pdf/sigir-2018-afm.pdf"><img src="/assets/img/afm.c5b7fa75.png" class="object-contain"></a></div><div class="mt-2 sm:mt-0 sm:ml-5 w-full"><p class="font-bold"><span>Attention-driven Factor Model for Explainable Personalized Recommendation</span></p><div class="text-sm"><p class="italic text-gray-700"><span><span>Jingwu Chen</span><span>, </span></span><span><span>Fuzhen Zhuang</span><span>, </span></span><span><span class="font-bold text-blue-900">Xin Hong</span><span>, </span></span><span><span>Xiang Ao</span><span>, </span></span><span><span>Xing Xie</span><span>, </span></span><span><span>Qing He</span><span>.</span></span></p><p class="text-gray-700"><span>ACM SIGIR Conference on Research and Development in Information Retrieval</span><span><span class="font-bold text-blue-900">
                    (SIGIR)</span><span>, 2018</span></span><!----><span>.</span></p></div><div class="flex text-lg text-gray-900"><a href="/pdf/sigir-2018-afm.pdf" class="block mr-2 hover:text-blue-800"><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" class="svg-inline--fa fa-file-pdf fa-w-12"><path fill="currentColor" d="M369.9 97.9L286 14C277 5 264.8-.1 252.1-.1H48C21.5 0 0 21.5 0 48v416c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V131.9c0-12.7-5.1-25-14.1-34zM332.1 128H256V51.9l76.1 76.1zM48 464V48h160v104c0 13.3 10.7 24 24 24h104v288H48zm250.2-143.7c-12.2-12-47-8.7-64.4-6.5-17.2-10.5-28.7-25-36.8-46.3 3.9-16.1 10.1-40.6 5.4-56-4.2-26.2-37.8-23.6-42.6-5.9-4.4 16.1-.4 38.5 7 67.1-10 23.9-24.9 56-35.4 74.4-20 10.3-47 26.2-51 46.2-3.3 15.8 26 55.2 76.1-31.2 22.4-7.4 46.8-16.5 68.4-20.1 18.9 10.2 41 17 55.8 17 25.5 0 28-28.2 17.5-38.7zm-198.1 77.8c5.1-13.7 24.5-29.5 30.4-35-19 30.3-30.4 35.7-30.4 35zm81.6-190.6c7.4 0 6.7 32.1 1.8 40.8-4.4-13.9-4.3-40.8-1.8-40.8zm-24.4 136.6c9.7-16.9 18-37 24.7-54.7 8.3 15.1 18.9 27.2 30.1 35.5-20.8 4.3-38.9 13.1-54.8 19.2zm131.6-5s-5 6-37.3-7.8c35.1-2.6 40.9 5.4 37.3 7.8z"></path></svg></a><!----><!----></div></div></div><!----></div></div></div><div><div class="px-5 py-16 max-w-3xl mx-auto flex justify-center"><div id="counter-wrap" class="w-64"></div></div></div></div>
    <script>window.__INITIAL_STATE__={"data":{"metadata":{"me":{"name":"Xin Hong","name_cn":"洪鑫","photo":"hongxin.jpg","email":"hongxin19b@ict.ac.cn","bio":"I am a Ph.D. student advised by \u003Ca class=\"link\" href=\"http:\u002F\u002Fwww.bigdatalab.ac.cn\u002F~lanyanyan\u002F\"\u003EYanyan Lan\u003C\u002Fa\u003E at the Institute of Computing Technology (ICT), Chinese Academy of Sciences. I work in the areas of computer vision and machine learning with focus on visual reasoning, and joint processing of text and image data.\n","cluster":{"publications":[{"title":"WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training","authors":["Yuqi Huo","Manli Zhang","Guangzhen Liu","Haoyu Lu","Yizhao Gao","Guoxing Yang","Jingyuan Wen","Heng Zhang","Baogui Xu","Weihao Zheng","Zongzheng Xi","Yueqian Yang","Anwen Hu","Jinming Zhao","Ruichen Li","Yida Zhao","Liang Zhang","Yuqing Song","Xin Hong","Wanqing Cui","Danyang Hou","Yingyan Li","Junyi Li","Peiyu Liu","Zheng Gong","Chuhao Jin","Yuchong Sun","Shizhe Chen","Zhiwu Lu","Zhicheng Dou","Qin Jin","Yanyan Lan","Wayne Xin Zhao","Ruihua Song","Ji-Rong Wen"],"proceeding":"Tec Report","proceeding_abbr":"","year":2021,"pdf":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F2103.06561.pdf","code":"","homepage":"","honor":"","img":"brivl.png"},{"title":"Transformation Driven Visual Reasoning","authors":["Xin Hong","Yanyan Lan","Liang Pang","Jiafeng Guo","Xueqi Cheng"],"proceeding":"The IEEE Conference on Computer Vision and Pattern Recognition","proceeding_abbr":"CVPR","year":2021,"pdf":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F2011.13160.pdf","code":"https:\u002F\u002Fgithub.com\u002Fhughplay\u002FTVR","homepage":"https:\u002F\u002Fhongxin2019.github.io\u002FTVR","honor":"","img":"tvr.png"},{"title":"Deep Fusion Network for Image Completion","authors":["Xin Hong","Pengfei Xiong","Haoqiang Fan"],"proceeding":"ACM International Conference on Multimedia","proceeding_abbr":"ACMMM","year":2019,"pdf":"\u002Fpdf\u002Fmm-2019-dfnet.pdf","code":"https:\u002F\u002Fgithub.com\u002Fhughplay\u002FDFNet","homepage":"","honor":"","img":"fusion-block.png"},{"title":"Attention-driven Factor Model for Explainable Personalized Recommendation","authors":["Jingwu Chen","Fuzhen Zhuang","Xin Hong","Xiang Ao","Xing Xie","Qing He"],"proceeding":"ACM SIGIR Conference on Research and Development in Information Retrieval","proceeding_abbr":"SIGIR","year":2018,"pdf":"\u002Fpdf\u002Fsigir-2018-afm.pdf","code":"","homepage":"","honor":"","img":"afm.png"}]}}}},"context":{}};(function(){var s;(s=document.currentScript||document.scripts[document.scripts.length-1]).parentNode.removeChild(s);}());</script><script src="/assets/js/app.967b8a34.js" defer></script><script src="/assets/js/page--src--pages--index-vue.d20a73fd.js" defer></script>
  </body>
</html>
