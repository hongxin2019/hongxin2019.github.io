<!DOCTYPE html>
<html data-html-server-rendered="true" lang="en" data-vue-tag="%7B%22lang%22:%7B%22ssr%22:%22en%22%7D%7D">
  <head>
    <title>Xin Hong</title><meta name="gridsome:hash" content="f4cf083e7bf4b35ec594c644c6efb1f51c137736"><meta data-vue-tag="ssr" charset="utf-8"><meta data-vue-tag="ssr" name="generator" content="Gridsome v0.7.23"><meta data-vue-tag="ssr" data-key="viewport" name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover"><meta data-vue-tag="ssr" data-key="format-detection" name="format-detection" content="telephone=no"><meta data-vue-tag="ssr" name="viewport" content="width=device-width, initial-scale=1"><meta data-vue-tag="ssr" data-key="description" name="description" content="I am a Ph.D. student advised by Yanyan Lan at the Institute of Computing Technology (ICT), Chinese Academy of Sciences. I work in the areas of computer vision and machine learning with focus on visual reasoning, and joint processing of vision and language. Before joining Yanyan Lan&#x27;s research group, I was an research intern at Megvii Technology working on image inpainting."><link data-vue-tag="ssr" rel="icon" href="data:,"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="16x16" href="/assets/static/favicon.ce0531f.c3a70b96b7e4797bf7eec6c39c6e02b6.png"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="32x32" href="/assets/static/favicon.ac8d93a.c3a70b96b7e4797bf7eec6c39c6e02b6.png"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="96x96" href="/assets/static/favicon.b9532cc.c3a70b96b7e4797bf7eec6c39c6e02b6.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="76x76" href="/assets/static/favicon.f22e9f3.c3a70b96b7e4797bf7eec6c39c6e02b6.png"><link rel="preload" href="/assets/css/3.styles.acae1a1c.css" as="style"><link rel="preload" href="/assets/js/app.eb9c576f.js" as="script"><link rel="preload" href="/assets/js/page--src--pages--index-vue.a55f753d.js" as="script"><link rel="prefetch" href="/assets/js/page--node-modules--gridsome--app--pages--404-vue.21072f36.js"><link rel="prefetch" href="/assets/js/vendors~page--src--pages--index-vue.2b3e265d.js"><link rel="stylesheet" href="/assets/css/3.styles.acae1a1c.css"><noscript data-vue-tag="ssr"><style>.g-image--loading{display:none;}</style></noscript>
  </head>
  <body >
    <div data-server-rendered="true" id="app" class="text-sm" style="line-height:normal;"><div class="border-b border-gray-300"><div class="flex flex-wrap px-5 py-16 mx-auto max-w-3xl sm:flex-no-wrap"><div class="w-40 mr-10"><img src="/assets/img/hongxin.25406a49.jpg" class="rounded shadow-lg"></div><div><p class="text-3xl mt-8 sm:mt-2"><span>Xin Hong</span><span>「洪鑫」</span></p><p class="mt-3 max-w-sm sm:max-w-md">I am a Ph.D. candidate advised by <a class="link" href="https://air.tsinghua.edu.cn/info/1046/1200.htm">Yanyan Lan</a> at the Institute of Computing Technology (ICT), Chinese Academy of Sciences. Currently, I work in the areas of computer vision and machine learning with focus on visual reasoning, and joint processing of text and image data.
</p><div class="mt-3 text-blue-900 flex"><p><a href="mailto:hongxin19b@ict.ac.cn" set="true" class="flex items-start">
              Email
            </a></p><p class="mx-1">/</p><p><a href="https://github.com/hughplay" set="true" class="flex items-start">
              Github
            </a></p><p class="mx-1">/</p><p><a href="https://scholar.google.com/citations?user=gW-9WOQAAAAJ&amp;hl=en" class="flex items-start">
              Google Scholar
            </a></p><p class="mx-1">/</p><p><a href="/hongxin_cv.pdf" class="flex items-start"> CV </a></p></div></div></div></div><div class="border-b border-gray-300"><div class="sm:px-0 px-5 my-16 max-w-3xl mx-auto"><p class="text-2xl mb-8 font-medium">Research</p><p class="my-8">
        I'm interested in computer vision and machine learning. Recently, I also try to apply machine learning techniques into biology problemcs, such as protein structure prediction.

      </p><div class="sm:flex py-5 sm:py-0"><div class="
            sm:p-5 sm:pl-0
            pl-0
            w-full
            sm:w-1/4
            flex flex-col
            justify-center
          "><img src="/assets/img/vtt.90c0152b.png"></div><div class="
            sm:p-5
            py-2
            sm:pr-0
            pr-0
            w-full
            sm:w-3/4
            flex flex-col
            justify-center
          "><p class="font-bold"><span>Visual Transformation Telling</span></p><div class="text-sm"><p><span><span class="font-bold">Xin Hong</span><span>, </span></span><span><span>Yanyan Lan</span><span>, </span></span><span><span>Liang Pang</span><span>, </span></span><span><span>Jiafeng Guo</span><span>, </span></span><span><span>Xueqi Cheng</span><!----></span></p><p><span class="italic"><span>under review</span></span><span>, 2022</span><!----></p></div><div class="flex items-center"><!----><!----><!----><!----></div></div></div><div class="sm:flex py-5 sm:py-0"><div class="
            sm:p-5 sm:pl-0
            pl-0
            w-full
            sm:w-1/4
            flex flex-col
            justify-center
          "><img src="/assets/img/tvr_illustration.1bc4ab21.png"></div><div class="
            sm:p-5
            py-2
            sm:pr-0
            pr-0
            w-full
            sm:w-3/4
            flex flex-col
            justify-center
          "><p class="font-bold"><span>Visual Reasoning: from State to Transformation</span></p><div class="text-sm"><p><span><span class="font-bold">Xin Hong</span><span>, </span></span><span><span>Yanyan Lan</span><span>, </span></span><span><span>Liang Pang</span><span>, </span></span><span><span>Jiafeng Guo</span><span>, </span></span><span><span>Xueqi Cheng</span><!----></span></p><p><span class="italic"><span>under review</span></span><span>, 2022</span><!----></p></div><div class="flex items-center"><!----><!----><!----><!----></div></div></div><div class="sm:flex py-5 sm:py-0"><div class="
            sm:p-5 sm:pl-0
            pl-0
            w-full
            sm:w-1/4
            flex flex-col
            justify-center
          "><img src="/assets/img/brivl.68921958.png"></div><div class="
            sm:p-5
            py-2
            sm:pr-0
            pr-0
            w-full
            sm:w-3/4
            flex flex-col
            justify-center
          "><p class="font-bold"><a href="https://arxiv.org/pdf/2103.06561.pdf"><span class="link">WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training</span></a></p><div class="text-sm"><p><span><span>Yuqi Huo</span><span>, </span></span><span><span>Manli Zhang</span><span>, </span></span><span><span>Guangzhen Liu</span><span>, </span></span><span><span>Haoyu Lu</span><span>, </span></span><span><span>Yizhao Gao</span><span>, </span></span><span><span>Guoxing Yang</span><span>, </span></span><span><span>Jingyuan Wen</span><span>, </span></span><span><span>Heng Zhang</span><span>, </span></span><span><span>Baogui Xu</span><span>, </span></span><span><span>Weihao Zheng</span><span>, </span></span><span><span>Zongzheng Xi</span><span>, </span></span><span><span>Yueqian Yang</span><span>, </span></span><span><span>Anwen Hu</span><span>, </span></span><span><span>Jinming Zhao</span><span>, </span></span><span><span>Ruichen Li</span><span>, </span></span><span><span>Yida Zhao</span><span>, </span></span><span><span>Liang Zhang</span><span>, </span></span><span><span>Yuqing Song</span><span>, </span></span><span><span class="font-bold">Xin Hong</span><span>, </span></span><span><span>Wanqing Cui</span><span>, </span></span><span><span>Danyang Hou</span><span>, </span></span><span><span>Yingyan Li</span><span>, </span></span><span><span>Junyi Li</span><span>, </span></span><span><span>Peiyu Liu</span><span>, </span></span><span><span>Zheng Gong</span><span>, </span></span><span><span>Chuhao Jin</span><span>, </span></span><span><span>Yuchong Sun</span><span>, </span></span><span><span>Shizhe Chen</span><span>, </span></span><span><span>Zhiwu Lu</span><span>, </span></span><span><span>Zhicheng Dou</span><span>, </span></span><span><span>Qin Jin</span><span>, </span></span><span><span>Yanyan Lan</span><span>, </span></span><span><span>Wayne Xin Zhao</span><span>, </span></span><span><span>Ruihua Song</span><span>, </span></span><span><span>Ji-Rong Wen</span><!----></span></p><p><span class="italic"><span>Tec Report</span></span><span>, 2021</span><!----></p></div><div class="flex items-center"><!----><!----><!----><div><!----><a href="https://arxiv.org/pdf/2103.06561.pdf"><span set="true">Paper</span></a></div></div></div></div><div class="sm:flex py-5 sm:py-0"><div class="
            sm:p-5 sm:pl-0
            pl-0
            w-full
            sm:w-1/4
            flex flex-col
            justify-center
          "><img src="/assets/img/tvr.0166b70c.png"></div><div class="
            sm:p-5
            py-2
            sm:pr-0
            pr-0
            w-full
            sm:w-3/4
            flex flex-col
            justify-center
          "><p class="font-bold"><a href="https://hongxin2019.github.io/TVR"><span class="link">Transformation Driven Visual Reasoning</span></a></p><div class="text-sm"><p><span><span class="font-bold">Xin Hong</span><span>, </span></span><span><span>Yanyan Lan</span><span>, </span></span><span><span>Liang Pang</span><span>, </span></span><span><span>Jiafeng Guo</span><span>, </span></span><span><span>Xueqi Cheng</span><!----></span></p><p><span class="italic"><span>CVPR</span></span><span>, 2021</span><!----></p></div><div class="flex items-center"><div><a href="https://hongxin2019.github.io/TVR"><span set="true">Project Page</span></a></div><div><span class="mx-1">/</span><a href="https://github.com/hughplay/TVR"><span set="true">Code</span></a></div><!----><div><span class="mx-1">/</span><a href="https://arxiv.org/pdf/2011.13160.pdf"><span set="true">Paper</span></a></div></div></div></div><div class="sm:flex py-5 sm:py-0"><div class="
            sm:p-5 sm:pl-0
            pl-0
            w-full
            sm:w-1/4
            flex flex-col
            justify-center
          "><img src="/assets/img/fusion-block.6fb11b23.png"></div><div class="
            sm:p-5
            py-2
            sm:pr-0
            pr-0
            w-full
            sm:w-3/4
            flex flex-col
            justify-center
          "><p class="font-bold"><a href="https://github.com/hughplay/DFNet"><span class="link">Deep Fusion Network for Image Completion</span></a></p><div class="text-sm"><p><span><span class="font-bold">Xin Hong</span><span>, </span></span><span><span>Pengfei Xiong</span><span>, </span></span><span><span>Renhe Ji</span><span>, </span></span><span><span>Haoqiang Fan</span><!----></span></p><p><span class="italic"><span>ACMMM</span></span><span>, 2019</span><!----></p></div><div class="flex items-center"><!----><div><!----><a href="https://github.com/hughplay/DFNet"><span set="true">Code</span></a></div><div><span class="mx-1">/</span><a href="https://colab.research.google.com/github/hughplay/DFNet/blob/master/demo.ipynb"><span set="true">Colab Demo</span></a></div><div><span class="mx-1">/</span><a href="/pdf/mm-2019-dfnet.pdf"><span set="true">Paper</span></a></div></div></div></div><div class="sm:flex py-5 sm:py-0"><div class="
            sm:p-5 sm:pl-0
            pl-0
            w-full
            sm:w-1/4
            flex flex-col
            justify-center
          "><img src="/assets/img/afm.c5b7fa75.png"></div><div class="
            sm:p-5
            py-2
            sm:pr-0
            pr-0
            w-full
            sm:w-3/4
            flex flex-col
            justify-center
          "><p class="font-bold"><a href="/pdf/sigir-2018-afm.pdf"><span class="link">Attention-driven Factor Model for Explainable Personalized Recommendation</span></a></p><div class="text-sm"><p><span><span>Jingwu Chen</span><span>, </span></span><span><span>Fuzhen Zhuang</span><span>, </span></span><span><span class="font-bold">Xin Hong</span><span>, </span></span><span><span>Xiang Ao</span><span>, </span></span><span><span>Xing Xie</span><span>, </span></span><span><span>Qing He</span><!----></span></p><p><span class="italic"><span>SIGIR</span></span><span>, 2018</span><!----></p></div><div class="flex items-center"><!----><!----><!----><div><!----><a href="/pdf/sigir-2018-afm.pdf"><span set="true">Paper</span></a></div></div></div></div></div><div class="sm:px-0 px-5 my-16 max-w-3xl mx-auto"><p class="text-2xl mb-4 font-medium">Project</p><div class="sm:flex py-5 sm:py-0"><div class="
            sm:p-5 sm:pl-0
            pl-0
            w-full
            sm:w-1/4
            flex flex-col
            justify-center
            scale-150
          "><img src="/assets/img/cameo_airfold.5718b67a.png"></div><div class="
            sm:p-5
            py-2
            sm:pr-0
            pr-0
            w-full
            sm:w-3/4
            flex flex-col
            justify-center
          "><p class="font-bold"><a href="https://mp.weixin.qq.com/s/ROeZYKvVJm-EBbhnDKZR1w"><span class="link">AIRFold</span></a></p><p class="mt-2">AIRFold ranked first in the CAMEO 3D structure prediction challenge for 4 weeks (2022.07.23-2022.08.20).</p></div></div><div class="sm:flex py-5 sm:py-0"><div class="
            sm:p-5 sm:pl-0
            pl-0
            w-full
            sm:w-1/4
            flex flex-col
            justify-center
            scale-150
          "><img src="/assets/img/deepcodebase_overview.f9516b36.png"></div><div class="
            sm:p-5
            py-2
            sm:pr-0
            pr-0
            w-full
            sm:w-3/4
            flex flex-col
            justify-center
          "><p class="font-bold"><a href="https://github.com/hughplay/DeepCodebase"><span class="link">DeepCodebase</span></a></p><p class="mt-2">DeepCodebase is a codebase / template for deep learning researchers, so that do experiments and releasing codes become easier.</p></div></div><div class="sm:flex py-5 sm:py-0"><div class="
            sm:p-5 sm:pl-0
            pl-0
            w-full
            sm:w-1/4
            flex flex-col
            justify-center
            scale-150
          "><img src="https://tacticalboard.github.io/img/shuttle.8e785e8d.jpg"></div><div class="
            sm:p-5
            py-2
            sm:pr-0
            pr-0
            w-full
            sm:w-3/4
            flex flex-col
            justify-center
          "><p class="font-bold"><a href="https://tacticalboard.github.io/"><span class="link">Tactic Board</span></a></p><p class="mt-2">A tactical board on your device for badminton, soccer, and basketball.</p></div></div></div><div class="sm:px-0 px-5 my-16 max-w-3xl mx-auto"><p class="text-2xl mb-4 font-medium">Review Experiences</p><p class="py-5">CVPR, ICCV, ECCV, AAAI, IJCV, JSTSP, Neural Computing and Applications.</p></div></div><div><div class="px-5 py-16 max-w-3xl mx-auto flex justify-center"><div id="counter-wrap" class="w-64"></div></div></div></div>
    <script>window.__INITIAL_STATE__={"data":{"metadata":{"me":{"name":"Xin Hong","name_cn":"洪鑫","photo":"hongxin.jpg","email":"hongxin19b@ict.ac.cn","github":"https:\u002F\u002Fgithub.com\u002Fhughplay","scholar":"https:\u002F\u002Fscholar.google.com\u002Fcitations?user=gW-9WOQAAAAJ&hl=en","cv":"\u002Fhongxin_cv.pdf","bio":"I am a Ph.D. candidate advised by \u003Ca class=\"link\" href=\"https:\u002F\u002Fair.tsinghua.edu.cn\u002Finfo\u002F1046\u002F1200.htm\"\u003EYanyan Lan\u003C\u002Fa\u003E at the Institute of Computing Technology (ICT), Chinese Academy of Sciences. Currently, I work in the areas of computer vision and machine learning with focus on visual reasoning, and joint processing of text and image data.\n","interests":"I'm interested in computer vision and machine learning. Recently, I also try to apply machine learning techniques into biology problemcs, such as protein structure prediction.\n","review_experiences":"CVPR, ICCV, ECCV, AAAI, IJCV, JSTSP, Neural Computing and Applications.","cluster":{"publications":[{"title":"Visual Transformation Telling","authors":["Xin Hong","Yanyan Lan","Liang Pang","Jiafeng Guo","Xueqi Cheng"],"proceeding":"under review","proceeding_abbr":"","year":2022,"homepage":"","code":"","notebook":"","pdf":null,"honor":"","img":"vtt.png","brief":""},{"title":"Visual Reasoning: from State to Transformation","authors":["Xin Hong","Yanyan Lan","Liang Pang","Jiafeng Guo","Xueqi Cheng"],"proceeding":"under review","proceeding_abbr":"","year":2022,"homepage":"","code":"","notebook":"","pdf":null,"honor":"","img":"tvr_illustration.png","brief":""},{"title":"WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training","authors":["Yuqi Huo","Manli Zhang","Guangzhen Liu","Haoyu Lu","Yizhao Gao","Guoxing Yang","Jingyuan Wen","Heng Zhang","Baogui Xu","Weihao Zheng","Zongzheng Xi","Yueqian Yang","Anwen Hu","Jinming Zhao","Ruichen Li","Yida Zhao","Liang Zhang","Yuqing Song","Xin Hong","Wanqing Cui","Danyang Hou","Yingyan Li","Junyi Li","Peiyu Liu","Zheng Gong","Chuhao Jin","Yuchong Sun","Shizhe Chen","Zhiwu Lu","Zhicheng Dou","Qin Jin","Yanyan Lan","Wayne Xin Zhao","Ruihua Song","Ji-Rong Wen"],"proceeding":"Tec Report","proceeding_abbr":"","year":2021,"homepage":"","code":"","notebook":"","pdf":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F2103.06561.pdf","honor":"","img":"brivl.png","brief":""},{"title":"Transformation Driven Visual Reasoning","authors":["Xin Hong","Yanyan Lan","Liang Pang","Jiafeng Guo","Xueqi Cheng"],"proceeding":"The IEEE Conference on Computer Vision and Pattern Recognition","proceeding_abbr":"CVPR","year":2021,"homepage":"https:\u002F\u002Fhongxin2019.github.io\u002FTVR","code":"https:\u002F\u002Fgithub.com\u002Fhughplay\u002FTVR","notebook":"","pdf":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F2011.13160.pdf","honor":"","img":"tvr.png","brief":""},{"title":"Deep Fusion Network for Image Completion","authors":["Xin Hong","Pengfei Xiong","Renhe Ji","Haoqiang Fan"],"proceeding":"ACM International Conference on Multimedia","proceeding_abbr":"ACMMM","year":2019,"homepage":"","code":"https:\u002F\u002Fgithub.com\u002Fhughplay\u002FDFNet","notebook":"https:\u002F\u002Fcolab.research.google.com\u002Fgithub\u002Fhughplay\u002FDFNet\u002Fblob\u002Fmaster\u002Fdemo.ipynb","pdf":"\u002Fpdf\u002Fmm-2019-dfnet.pdf","honor":"","img":"fusion-block.png","brief":""},{"title":"Attention-driven Factor Model for Explainable Personalized Recommendation","authors":["Jingwu Chen","Fuzhen Zhuang","Xin Hong","Xiang Ao","Xing Xie","Qing He"],"proceeding":"ACM SIGIR Conference on Research and Development in Information Retrieval","proceeding_abbr":"SIGIR","year":2018,"homepage":"","code":"","notebook":"","pdf":"\u002Fpdf\u002Fsigir-2018-afm.pdf","honor":"","img":"afm.png","brief":""}],"projects":[{"title":"AIRFold","code":"","homepage":"https:\u002F\u002Fmp.weixin.qq.com\u002Fs\u002FROeZYKvVJm-EBbhnDKZR1w","img":"cameo_airfold.png","brief":"AIRFold ranked first in the CAMEO 3D structure prediction challenge for 4 weeks (2022.07.23-2022.08.20)."},{"title":"DeepCodebase","code":"https:\u002F\u002Fgithub.com\u002Fhughplay\u002FDeepCodebase","homepage":"","img":"deepcodebase_overview.png","brief":"DeepCodebase is a codebase \u002F template for deep learning researchers, so that do experiments and releasing codes become easier."},{"title":"Tactic Board","code":"https:\u002F\u002Fgithub.com\u002Fhughplay\u002FDeepCodebase","homepage":"https:\u002F\u002Ftacticalboard.github.io\u002F","img":"https:\u002F\u002Ftacticalboard.github.io\u002Fimg\u002Fshuttle.8e785e8d.jpg","brief":"A tactical board on your device for badminton, soccer, and basketball."}]}}}},"context":{}};(function(){var s;(s=document.currentScript||document.scripts[document.scripts.length-1]).parentNode.removeChild(s);}());</script><script src="/assets/js/app.eb9c576f.js" defer></script><script src="/assets/js/page--src--pages--index-vue.a55f753d.js" defer></script>
  </body>
</html>
