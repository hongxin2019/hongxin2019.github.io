<!DOCTYPE html>
<html data-html-server-rendered="true" lang="en" data-vue-tag="%7B%22lang%22:%7B%22ssr%22:%22en%22%7D%7D">
  <head>
    <title>Xin Hong</title><meta name="gridsome:hash" content="c0aca85fa289abc9d596b420d7540f610780a85e"><meta data-vue-tag="ssr" charset="utf-8"><meta data-vue-tag="ssr" name="generator" content="Gridsome v0.7.23"><meta data-vue-tag="ssr" data-key="viewport" name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover"><meta data-vue-tag="ssr" data-key="format-detection" name="format-detection" content="telephone=no"><meta data-vue-tag="ssr" name="viewport" content="width=device-width, initial-scale=1"><meta data-vue-tag="ssr" data-key="description" name="description" content="I am a Ph.D. student advised by Yanyan Lan at the Institute of Computing Technology (ICT), Chinese Academy of Sciences. I work in the areas of computer vision and machine learning with focus on visual reasoning, and joint processing of vision and language. Before joining Yanyan Lan&#x27;s research group, I was an research intern at Megvii Technology working on image inpainting."><link data-vue-tag="ssr" rel="icon" href="data:,"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="16x16" href="/assets/static/favicon.ce0531f.c3a70b96b7e4797bf7eec6c39c6e02b6.png"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="32x32" href="/assets/static/favicon.ac8d93a.c3a70b96b7e4797bf7eec6c39c6e02b6.png"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="96x96" href="/assets/static/favicon.b9532cc.c3a70b96b7e4797bf7eec6c39c6e02b6.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="76x76" href="/assets/static/favicon.f22e9f3.c3a70b96b7e4797bf7eec6c39c6e02b6.png"><link rel="preload" href="/assets/css/3.styles.da7f8fce.css" as="style"><link rel="preload" href="/assets/js/app.d571e0ba.js" as="script"><link rel="preload" href="/assets/js/page--src--pages--index-vue.2de3437f.js" as="script"><link rel="prefetch" href="/assets/js/page--node-modules--gridsome--app--pages--404-vue.14ff9da7.js"><link rel="prefetch" href="/assets/js/vendors~page--src--pages--index-vue.2b3e265d.js"><link rel="stylesheet" href="/assets/css/3.styles.da7f8fce.css"><noscript data-vue-tag="ssr"><style>.g-image--loading{display:none;}</style></noscript>
  </head>
  <body >
    <div data-server-rendered="true" id="app" class="text-sm" style="line-height:normal;"><div class="border-b border-gray-300"><div class="flex flex-wrap px-5 py-16 mx-auto max-w-3xl sm:flex-no-wrap"><div class="w-40 mr-10"><img src="/assets/img/hongxin.25406a49.jpg" class="rounded shadow-lg object-contain"></div><div><p class="text-3xl mt-8 sm:mt-2"><span>Xin Hong</span><span>「洪鑫」</span></p><p class="text-blue-900 font-medium font-mono">
          hongxin19b@ict.ac.cn
        </p><p class="mt-3 max-w-sm sm:max-w-md">I am a Ph.D. candidate advised by <a class="link" href="http://www.bigdatalab.ac.cn/~lanyanyan/">Yanyan Lan</a> at the Institute of Computing Technology (ICT), Chinese Academy of Sciences. Currently, he works in the areas of computer vision and machine learning with focus on visual reasoning, and joint processing of text and image data.
</p><div class="mt-3 text-blue-900 flex"><p class="mr-3"><a href="https://github.com/hughplay" class="flex items-start"><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" fill="#2a4365" width="1.25em" height="1.25em" class="mr-1"><title>GitHub icon</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg>
              Github
            </a></p><p><a href="https://scholar.google.com/citations?user=gW-9WOQAAAAJ&amp;hl=en" class="flex items-start"><svg role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="#2a4365" width="1.25em" height="1.25em" class="mr-1"><title>Google Scholar icon</title><path d="M12 24a7 7 0 1 1 0-14 7 7 0 0 1 0 14zm0-24L0 9.5l4.838 3.94A8 8 0 0 1 12 9a8 8 0 0 1 7.162 4.44L24 9.5z"></path></svg>
              Google Scholar
            </a></p></div></div></div></div><div class="border-b border-gray-300"><div class="px-5 py-16 max-w-3xl mx-auto"><p class="text-xl mb-8 font-medium">Research</p><div class="mt-8"><div class="sm:flex"><div class="w-64 sm:w-64"><a href="https://arxiv.org/pdf/2103.06561.pdf"><img src="/assets/img/brivl.68921958.png" class="object-contain"></a></div><div class="mt-2 sm:mt-0 sm:ml-5 w-full"><p class="font-bold"><span>WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training</span></p><div class="text-sm"><p class="text-gray-700"><span><span>Yuqi Huo</span><span>, </span></span><span><span>Manli Zhang</span><span>, </span></span><span><span>Guangzhen Liu</span><span>, </span></span><span><span>Haoyu Lu</span><span>, </span></span><span><span>Yizhao Gao</span><span>, </span></span><span><span>Guoxing Yang</span><span>, </span></span><span><span>Jingyuan Wen</span><span>, </span></span><span><span>Heng Zhang</span><span>, </span></span><span><span>Baogui Xu</span><span>, </span></span><span><span>Weihao Zheng</span><span>, </span></span><span><span>Zongzheng Xi</span><span>, </span></span><span><span>Yueqian Yang</span><span>, </span></span><span><span>Anwen Hu</span><span>, </span></span><span><span>Jinming Zhao</span><span>, </span></span><span><span>Ruichen Li</span><span>, </span></span><span><span>Yida Zhao</span><span>, </span></span><span><span>Liang Zhang</span><span>, </span></span><span><span>Yuqing Song</span><span>, </span></span><span><span class="font-bold">Xin Hong</span><span>, </span></span><span><span>Wanqing Cui</span><span>, </span></span><span><span>Danyang Hou</span><span>, </span></span><span><span>Yingyan Li</span><span>, </span></span><span><span>Junyi Li</span><span>, </span></span><span><span>Peiyu Liu</span><span>, </span></span><span><span>Zheng Gong</span><span>, </span></span><span><span>Chuhao Jin</span><span>, </span></span><span><span>Yuchong Sun</span><span>, </span></span><span><span>Shizhe Chen</span><span>, </span></span><span><span>Zhiwu Lu</span><span>, </span></span><span><span>Zhicheng Dou</span><span>, </span></span><span><span>Qin Jin</span><span>, </span></span><span><span>Yanyan Lan</span><span>, </span></span><span><span>Wayne Xin Zhao</span><span>, </span></span><span><span>Ruihua Song</span><span>, </span></span><span><span>Ji-Rong Wen</span><span>.</span></span></p><p class="text-gray-700"><span class="italic"><span>Tec Report</span></span><span>,2021</span><!----></p></div><div class="flex text-gray-900 items-center mt-2 pl-1"><!----><!----><!----><a href="https://arxiv.org/pdf/2103.06561.pdf" class="block mr-3 hover:text-blue-800 flex items-center"><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="1.2em" height="1.2em"><title>Adobe Acrobat Reader icon</title><path d="M23.63 15.3c-.71-.745-2.166-1.17-4.224-1.17-1.1 0-2.377.106-3.761.354a19.443 19.443 0 0 1-2.307-2.661c-.532-.71-.994-1.49-1.42-2.236.817-2.484 1.207-4.507 1.207-5.962 0-1.632-.603-3.336-2.342-3.336-.532 0-1.065.32-1.349.781-.78 1.384-.425 4.4.923 7.381a60.277 60.277 0 0 1-1.703 4.507c-.568 1.349-1.207 2.733-1.917 4.01C2.834 18.53.314 20.34.03 21.758c-.106.533.071 1.03.462 1.42.142.107.639.533 1.49.533 2.59 0 5.323-4.188 6.707-6.707 1.065-.355 2.13-.71 3.194-.994a34.963 34.963 0 0 1 3.407-.745c2.732 2.448 5.145 2.839 6.352 2.839 1.49 0 2.023-.604 2.2-1.1.32-.64.106-1.349-.213-1.704zm-1.42 1.03c-.107.532-.64.887-1.384.887-.213 0-.39-.036-.604-.071-1.348-.32-2.626-.994-3.903-2.059a17.717 17.717 0 0 1 2.98-.248c.746 0 1.385.035 1.81.142.497.106 1.278.426 1.1 1.348zm-7.524-1.668a38.01 38.01 0 0 0-2.945.674 39.68 39.68 0 0 0-2.52.745 40.05 40.05 0 0 0 1.207-2.555c.426-.994.78-2.023 1.136-2.981.354.603.745 1.207 1.135 1.739a50.127 50.127 0 0 0 1.987 2.378zM10.038 1.46a.768.768 0 0 1 .674-.425c.745 0 .887.851.887 1.526 0 1.135-.355 2.874-.958 4.861-1.03-2.768-1.1-5.074-.603-5.962zM6.134 17.997c-1.81 2.981-3.549 4.826-4.613 4.826a.872.872 0 0 1-.532-.177c-.213-.213-.32-.461-.249-.745.213-1.065 2.271-2.555 5.394-3.904Z"></path></svg></a></div></div></div><div class="border-b h-8 w-full"></div></div><div class="mt-8"><div class="sm:flex"><div class="w-64 sm:w-64"><a href="https://hongxin2019.github.io/TVR"><img src="/assets/img/tvr.0166b70c.png" class="object-contain"></a></div><div class="mt-2 sm:mt-0 sm:ml-5 w-full"><p class="font-bold"><a href="https://hongxin2019.github.io/TVR"><span class="link">Transformation Driven Visual Reasoning</span></a></p><div class="text-sm"><p class="text-gray-700"><span><span class="font-bold">Xin Hong</span><span>, </span></span><span><span>Yanyan Lan</span><span>, </span></span><span><span>Liang Pang</span><span>, </span></span><span><span>Jiafeng Guo</span><span>, </span></span><span><span>Xueqi Cheng</span><span>.</span></span></p><p class="text-gray-700"><span class="italic"><span>CVPR</span></span><span>,2021</span><!----></p></div><div class="flex text-gray-900 items-center mt-2 pl-1"><a href="https://hongxin2019.github.io/TVR" class="block mr-3 hover:text-blue-800 flex items-center"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="home" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512" class="text-lg svg-inline--fa fa-home fa-w-18"><path fill="currentColor" d="M280.37 148.26L96 300.11V464a16 16 0 0 0 16 16l112.06-.29a16 16 0 0 0 15.92-16V368a16 16 0 0 1 16-16h64a16 16 0 0 1 16 16v95.64a16 16 0 0 0 16 16.05L464 480a16 16 0 0 0 16-16V300L295.67 148.26a12.19 12.19 0 0 0-15.3 0zM571.6 251.47L488 182.56V44.05a12 12 0 0 0-12-12h-56a12 12 0 0 0-12 12v72.61L318.47 43a48 48 0 0 0-61 0L4.34 251.47a12 12 0 0 0-1.6 16.9l25.5 31A12 12 0 0 0 45.15 301l235.22-193.74a12.19 12.19 0 0 1 15.3 0L530.9 301a12 12 0 0 0 16.9-1.6l25.5-31a12 12 0 0 0-1.7-16.93z"></path></svg></a><a href="https://github.com/hughplay/TVR" class="block mr-3 hover:text-blue-800 flex items-center"><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="1.2em" height="1.2em" class="custom-class"><title>GitHub icon</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg></a><!----><a href="https://arxiv.org/pdf/2011.13160.pdf" class="block mr-3 hover:text-blue-800 flex items-center"><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="1.2em" height="1.2em"><title>Adobe Acrobat Reader icon</title><path d="M23.63 15.3c-.71-.745-2.166-1.17-4.224-1.17-1.1 0-2.377.106-3.761.354a19.443 19.443 0 0 1-2.307-2.661c-.532-.71-.994-1.49-1.42-2.236.817-2.484 1.207-4.507 1.207-5.962 0-1.632-.603-3.336-2.342-3.336-.532 0-1.065.32-1.349.781-.78 1.384-.425 4.4.923 7.381a60.277 60.277 0 0 1-1.703 4.507c-.568 1.349-1.207 2.733-1.917 4.01C2.834 18.53.314 20.34.03 21.758c-.106.533.071 1.03.462 1.42.142.107.639.533 1.49.533 2.59 0 5.323-4.188 6.707-6.707 1.065-.355 2.13-.71 3.194-.994a34.963 34.963 0 0 1 3.407-.745c2.732 2.448 5.145 2.839 6.352 2.839 1.49 0 2.023-.604 2.2-1.1.32-.64.106-1.349-.213-1.704zm-1.42 1.03c-.107.532-.64.887-1.384.887-.213 0-.39-.036-.604-.071-1.348-.32-2.626-.994-3.903-2.059a17.717 17.717 0 0 1 2.98-.248c.746 0 1.385.035 1.81.142.497.106 1.278.426 1.1 1.348zm-7.524-1.668a38.01 38.01 0 0 0-2.945.674 39.68 39.68 0 0 0-2.52.745 40.05 40.05 0 0 0 1.207-2.555c.426-.994.78-2.023 1.136-2.981.354.603.745 1.207 1.135 1.739a50.127 50.127 0 0 0 1.987 2.378zM10.038 1.46a.768.768 0 0 1 .674-.425c.745 0 .887.851.887 1.526 0 1.135-.355 2.874-.958 4.861-1.03-2.768-1.1-5.074-.603-5.962zM6.134 17.997c-1.81 2.981-3.549 4.826-4.613 4.826a.872.872 0 0 1-.532-.177c-.213-.213-.32-.461-.249-.745.213-1.065 2.271-2.555 5.394-3.904Z"></path></svg></a></div></div></div><div class="border-b h-8 w-full"></div></div><div class="mt-8"><div class="sm:flex"><div class="w-64 sm:w-64"><a href="/pdf/mm-2019-dfnet.pdf"><img src="/assets/img/fusion-block.6fb11b23.png" class="object-contain"></a></div><div class="mt-2 sm:mt-0 sm:ml-5 w-full"><p class="font-bold"><span>Deep Fusion Network for Image Completion</span></p><div class="text-sm"><p class="text-gray-700"><span><span class="font-bold">Xin Hong</span><span>, </span></span><span><span>Pengfei Xiong</span><span>, </span></span><span><span>Renhe Ji</span><span>, </span></span><span><span>Haoqiang Fan</span><span>.</span></span></p><p class="text-gray-700"><span class="italic"><span>ACMMM</span></span><span>,2019</span><!----></p></div><div class="flex text-gray-900 items-center mt-2 pl-1"><!----><a href="https://github.com/hughplay/DFNet" class="block mr-3 hover:text-blue-800 flex items-center"><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="1.2em" height="1.2em" class="custom-class"><title>GitHub icon</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg></a><a href="https://colab.research.google.com/github/hughplay/DFNet/blob/master/demo.ipynb" class="block mr-3 hover:text-blue-800 flex items-center"><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="1.25em" height="1.25em"><title>Google Colab icon</title><path d="M16.9414 4.9757a7.033 7.033 0 0 0-4.9308 2.0646 7.033 7.033 0 0 0-.1232 9.8068l2.395-2.395a3.6455 3.6455 0 0 1 5.1497-5.1478l2.397-2.3989a7.033 7.033 0 0 0-4.8877-1.9297zM7.07 4.9855a7.033 7.033 0 0 0-4.8878 1.9316l2.3911 2.3911a3.6434 3.6434 0 0 1 5.0227.1271l1.7341-2.9737-.0997-.0802A7.033 7.033 0 0 0 7.07 4.9855zm15.0093 2.1721l-2.3892 2.3911a3.6455 3.6455 0 0 1-5.1497 5.1497l-2.4067 2.4068a7.0362 7.0362 0 0 0 9.9456-9.9476zM1.932 7.1674a7.033 7.033 0 0 0-.002 9.6816l2.397-2.397a3.6434 3.6434 0 0 1-.004-4.8916zm7.664 7.4235c-1.38 1.3816-3.5863 1.411-5.0168.1134l-2.397 2.395c2.4693 2.3328 6.263 2.5753 9.0072.5455l.1368-.1115z"></path></svg></a><a href="/pdf/mm-2019-dfnet.pdf" class="block mr-3 hover:text-blue-800 flex items-center"><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="1.2em" height="1.2em"><title>Adobe Acrobat Reader icon</title><path d="M23.63 15.3c-.71-.745-2.166-1.17-4.224-1.17-1.1 0-2.377.106-3.761.354a19.443 19.443 0 0 1-2.307-2.661c-.532-.71-.994-1.49-1.42-2.236.817-2.484 1.207-4.507 1.207-5.962 0-1.632-.603-3.336-2.342-3.336-.532 0-1.065.32-1.349.781-.78 1.384-.425 4.4.923 7.381a60.277 60.277 0 0 1-1.703 4.507c-.568 1.349-1.207 2.733-1.917 4.01C2.834 18.53.314 20.34.03 21.758c-.106.533.071 1.03.462 1.42.142.107.639.533 1.49.533 2.59 0 5.323-4.188 6.707-6.707 1.065-.355 2.13-.71 3.194-.994a34.963 34.963 0 0 1 3.407-.745c2.732 2.448 5.145 2.839 6.352 2.839 1.49 0 2.023-.604 2.2-1.1.32-.64.106-1.349-.213-1.704zm-1.42 1.03c-.107.532-.64.887-1.384.887-.213 0-.39-.036-.604-.071-1.348-.32-2.626-.994-3.903-2.059a17.717 17.717 0 0 1 2.98-.248c.746 0 1.385.035 1.81.142.497.106 1.278.426 1.1 1.348zm-7.524-1.668a38.01 38.01 0 0 0-2.945.674 39.68 39.68 0 0 0-2.52.745 40.05 40.05 0 0 0 1.207-2.555c.426-.994.78-2.023 1.136-2.981.354.603.745 1.207 1.135 1.739a50.127 50.127 0 0 0 1.987 2.378zM10.038 1.46a.768.768 0 0 1 .674-.425c.745 0 .887.851.887 1.526 0 1.135-.355 2.874-.958 4.861-1.03-2.768-1.1-5.074-.603-5.962zM6.134 17.997c-1.81 2.981-3.549 4.826-4.613 4.826a.872.872 0 0 1-.532-.177c-.213-.213-.32-.461-.249-.745.213-1.065 2.271-2.555 5.394-3.904Z"></path></svg></a></div></div></div><div class="border-b h-8 w-full"></div></div><div class="mt-8"><div class="sm:flex"><div class="w-64 sm:w-64"><a href="/pdf/sigir-2018-afm.pdf"><img src="/assets/img/afm.c5b7fa75.png" class="object-contain"></a></div><div class="mt-2 sm:mt-0 sm:ml-5 w-full"><p class="font-bold"><span>Attention-driven Factor Model for Explainable Personalized Recommendation</span></p><div class="text-sm"><p class="text-gray-700"><span><span>Jingwu Chen</span><span>, </span></span><span><span>Fuzhen Zhuang</span><span>, </span></span><span><span class="font-bold">Xin Hong</span><span>, </span></span><span><span>Xiang Ao</span><span>, </span></span><span><span>Xing Xie</span><span>, </span></span><span><span>Qing He</span><span>.</span></span></p><p class="text-gray-700"><span class="italic"><span>SIGIR</span></span><span>,2018</span><!----></p></div><div class="flex text-gray-900 items-center mt-2 pl-1"><!----><!----><!----><a href="/pdf/sigir-2018-afm.pdf" class="block mr-3 hover:text-blue-800 flex items-center"><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" width="1.2em" height="1.2em"><title>Adobe Acrobat Reader icon</title><path d="M23.63 15.3c-.71-.745-2.166-1.17-4.224-1.17-1.1 0-2.377.106-3.761.354a19.443 19.443 0 0 1-2.307-2.661c-.532-.71-.994-1.49-1.42-2.236.817-2.484 1.207-4.507 1.207-5.962 0-1.632-.603-3.336-2.342-3.336-.532 0-1.065.32-1.349.781-.78 1.384-.425 4.4.923 7.381a60.277 60.277 0 0 1-1.703 4.507c-.568 1.349-1.207 2.733-1.917 4.01C2.834 18.53.314 20.34.03 21.758c-.106.533.071 1.03.462 1.42.142.107.639.533 1.49.533 2.59 0 5.323-4.188 6.707-6.707 1.065-.355 2.13-.71 3.194-.994a34.963 34.963 0 0 1 3.407-.745c2.732 2.448 5.145 2.839 6.352 2.839 1.49 0 2.023-.604 2.2-1.1.32-.64.106-1.349-.213-1.704zm-1.42 1.03c-.107.532-.64.887-1.384.887-.213 0-.39-.036-.604-.071-1.348-.32-2.626-.994-3.903-2.059a17.717 17.717 0 0 1 2.98-.248c.746 0 1.385.035 1.81.142.497.106 1.278.426 1.1 1.348zm-7.524-1.668a38.01 38.01 0 0 0-2.945.674 39.68 39.68 0 0 0-2.52.745 40.05 40.05 0 0 0 1.207-2.555c.426-.994.78-2.023 1.136-2.981.354.603.745 1.207 1.135 1.739a50.127 50.127 0 0 0 1.987 2.378zM10.038 1.46a.768.768 0 0 1 .674-.425c.745 0 .887.851.887 1.526 0 1.135-.355 2.874-.958 4.861-1.03-2.768-1.1-5.074-.603-5.962zM6.134 17.997c-1.81 2.981-3.549 4.826-4.613 4.826a.872.872 0 0 1-.532-.177c-.213-.213-.32-.461-.249-.745.213-1.065 2.271-2.555 5.394-3.904Z"></path></svg></a></div></div></div><!----></div></div></div><div><div class="px-5 py-16 max-w-3xl mx-auto flex justify-center"><div id="counter-wrap" class="w-64"></div></div></div></div>
    <script>window.__INITIAL_STATE__={"data":{"metadata":{"me":{"name":"Xin Hong","name_cn":"洪鑫","photo":"hongxin.jpg","email":"hongxin19b@ict.ac.cn","github":"https:\u002F\u002Fgithub.com\u002Fhughplay","scholar":"https:\u002F\u002Fscholar.google.com\u002Fcitations?user=gW-9WOQAAAAJ&hl=en","bio":"I am a Ph.D. candidate advised by \u003Ca class=\"link\" href=\"http:\u002F\u002Fwww.bigdatalab.ac.cn\u002F~lanyanyan\u002F\"\u003EYanyan Lan\u003C\u002Fa\u003E at the Institute of Computing Technology (ICT), Chinese Academy of Sciences. Currently, he works in the areas of computer vision and machine learning with focus on visual reasoning, and joint processing of text and image data.\n","cluster":{"publications":[{"title":"WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training","authors":["Yuqi Huo","Manli Zhang","Guangzhen Liu","Haoyu Lu","Yizhao Gao","Guoxing Yang","Jingyuan Wen","Heng Zhang","Baogui Xu","Weihao Zheng","Zongzheng Xi","Yueqian Yang","Anwen Hu","Jinming Zhao","Ruichen Li","Yida Zhao","Liang Zhang","Yuqing Song","Xin Hong","Wanqing Cui","Danyang Hou","Yingyan Li","Junyi Li","Peiyu Liu","Zheng Gong","Chuhao Jin","Yuchong Sun","Shizhe Chen","Zhiwu Lu","Zhicheng Dou","Qin Jin","Yanyan Lan","Wayne Xin Zhao","Ruihua Song","Ji-Rong Wen"],"proceeding":"Tec Report","proceeding_abbr":"","year":2021,"homepage":"","code":"","notebook":"","pdf":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F2103.06561.pdf","honor":"","img":"brivl.png"},{"title":"Transformation Driven Visual Reasoning","authors":["Xin Hong","Yanyan Lan","Liang Pang","Jiafeng Guo","Xueqi Cheng"],"proceeding":"The IEEE Conference on Computer Vision and Pattern Recognition","proceeding_abbr":"CVPR","year":2021,"homepage":"https:\u002F\u002Fhongxin2019.github.io\u002FTVR","code":"https:\u002F\u002Fgithub.com\u002Fhughplay\u002FTVR","notebook":"","pdf":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F2011.13160.pdf","honor":"","img":"tvr.png"},{"title":"Deep Fusion Network for Image Completion","authors":["Xin Hong","Pengfei Xiong","Renhe Ji","Haoqiang Fan"],"proceeding":"ACM International Conference on Multimedia","proceeding_abbr":"ACMMM","year":2019,"homepage":"","code":"https:\u002F\u002Fgithub.com\u002Fhughplay\u002FDFNet","notebook":"https:\u002F\u002Fcolab.research.google.com\u002Fgithub\u002Fhughplay\u002FDFNet\u002Fblob\u002Fmaster\u002Fdemo.ipynb","pdf":"\u002Fpdf\u002Fmm-2019-dfnet.pdf","honor":"","img":"fusion-block.png"},{"title":"Attention-driven Factor Model for Explainable Personalized Recommendation","authors":["Jingwu Chen","Fuzhen Zhuang","Xin Hong","Xiang Ao","Xing Xie","Qing He"],"proceeding":"ACM SIGIR Conference on Research and Development in Information Retrieval","proceeding_abbr":"SIGIR","year":2018,"homepage":"","code":"","notebook":"","pdf":"\u002Fpdf\u002Fsigir-2018-afm.pdf","honor":"","img":"afm.png"}]}}}},"context":{}};(function(){var s;(s=document.currentScript||document.scripts[document.scripts.length-1]).parentNode.removeChild(s);}());</script><script src="/assets/js/app.d571e0ba.js" defer></script><script src="/assets/js/page--src--pages--index-vue.2de3437f.js" defer></script>
  </body>
</html>
